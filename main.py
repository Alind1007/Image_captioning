import torch
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import os
import requests
from io import BytesIO
from PIL import Image
from google.colab import drive
from transformers import BlipProcessor, BlipForConditionalGeneration, GPT2LMHeadModel, GPT2Tokenizer

# Mount Google Drive (for accessing Drive images)
drive.mount('/content/drive')

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load BLIP model for feature extraction
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
blip_model.eval()

# Load GPT-2 for caption refinement
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
gpt2_model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((384, 384)),
    transforms.ToTensor(),
])

# Function to download and save an image from a URL
def download_image(image_url):
    try:
        response = requests.get(image_url, stream=True)
        response.raise_for_status()  # Check if request was successful
        img = Image.open(BytesIO(response.content))
        
        # Save image locally
        image_path = "downloaded_image.jpg"
        img.save(image_path)

        print(f"‚úÖ Image downloaded successfully: {image_path}")
        return image_path  # Return local path
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error: Could not download image! {e}")
        return None

# Function to generate captions using BLIP
def generate_blip_caption(image_path):
    if not os.path.exists(image_path):
        print("‚ùå Error: Image file not found!")
        return None
    
    # Load and display image
    image = Image.open(image_path).convert("RGB")
    plt.imshow(image)
    plt.axis("off")
    plt.show()
    
    # Extract features and generate caption using BLIP
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        blip_features = blip_model.generate(**inputs)
    
    # Decode BLIP output
    blip_caption = processor.decode(blip_features[0], skip_special_tokens=True)
    
    print("\nüìå *BLIP Initial Caption:*", blip_caption)
    return blip_caption

# Function to refine the caption using GPT-2
def refine_caption(blip_caption):
    if blip_caption is None:
        return "‚ùå Error: No caption available to refine!"

    input_text = "Refine this caption: " + blip_caption
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

    with torch.no_grad():
        output = gpt2_model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)

    refined_caption = tokenizer.decode(output[0], skip_special_tokens=True).replace("Refine this caption:", "").strip()

    print("\nüìå *Final Refined Caption (GPT-2):*", refined_caption)
    return refined_caption

# Get user input (URL or Google Drive file path)
image_input = input("Enter the path to an image (Google Drive Path or Online URL): ")

# Check if the input is a URL
if image_input.startswith("http"):
    image_path = download_image(image_input)  # Download and get the local path
elif image_input.startswith("/content/drive"):
    image_path = image_input  # Directly use the Google Drive path
else:
    image_path = image_input  # Assume it's a local file path

# Generate caption if the image exists
if image_path and os.path.exists(image_path):
    blip_caption = generate_blip_caption(image_path)
    refined_caption = refine_caption(blip_caption)
else:
    print("‚ùå Error: Image file not found!")
